{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPocvuXkIEZTAgK21q0IC49",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luaim/Plant-Species-Identification-Model/blob/main/Plant_Species_Identification_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± Plant Identification Model (MobileNetV2, TensorFlow/Keras, TFLite Export)\n",
        "# Author: Luai - GIthup: Luaim\n",
        "# Description: Full pipeline - Data, Training, Evaluation, TFLite Export for Flutter app\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "bz8SAkvxC6JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install & Import Required Libraries\n",
        "\n",
        "We start by installing and importing all the required Python libraries.  \n",
        "This includes TensorFlow (for building and training the deep learning model),  \n",
        "NumPy, Pandas, Matplotlib, and Seaborn (for data manipulation and visualization),  \n",
        "and scikit-learn (for evaluation).  \n",
        "We will use MobileNetV2 as our backbone for transfer learning.\n"
      ],
      "metadata": {
        "id": "QbBZkyTpDKMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install & Import Libraries\n",
        "!pip install tensorflow seaborn --quiet\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "blF3BpJWDQOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Mount Google Drive and Set Data Paths\n",
        "\n",
        "We mount Google Drive so we can access our dataset and JSON files stored there.  \n",
        "Update the path variables (`DATA_DIR` and `JSON_PATH`) to match your folder structure in Drive.\n"
      ],
      "metadata": {
        "id": "PZ_BVnqzDRMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# SET YOUR PATHS HERE:\n",
        "DATA_DIR = \"/content/drive/MyDrive/plantnet_300K/images\"\n",
        "JSON_PATH = \"/content/drive/MyDrive/plantnet_300K/plantnet300k_inf.json\"\n"
      ],
      "metadata": {
        "id": "bhQioP-TDRxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare Label Mapping from JSON\n",
        "\n",
        "We read the metadata JSON file to create a mapping from image hashes to `species_id` and  \n",
        "from `species_id` to readable species names.  \n",
        "This will help us match images to their correct labels and display plant names in the app.\n"
      ],
      "metadata": {
        "id": "UEE5UpV4DTgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Prepare Label Mapping from JSON\n",
        "with open(JSON_PATH, 'r') as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "image_hash_to_species = {k: v[\"species_id\"] for k,v in meta.items()}\n",
        "species_id_to_name = {v[\"species_id\"]: v[\"species_name\"] for v in meta.values()}\n",
        "species_ids = sorted(list(set(image_hash_to_species.values())))\n",
        "species_id_to_index = {sid: i for i, sid in enumerate(species_ids)}\n"
      ],
      "metadata": {
        "id": "vIQxX9LIDUHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ## Step 4: Create Data Generators\n",
        "\n",
        "We use Keras' `ImageDataGenerator` to load images from folders, automatically labeling them  \n",
        "based on the folder names (`class_id`).  \n",
        "The generator also performs real-time data augmentation for the training set to help the model generalize better.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "riVDOq9VDWqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Data Generators\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(DATA_DIR, 'train'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(DATA_DIR, 'val'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(DATA_DIR, 'test'),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "YnnSq2SfDXEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Build the Transfer Learning Model (MobileNetV2)\n",
        "\n",
        "We create our model using MobileNetV2 as a feature extractor (pretrained on ImageNet).  \n",
        "We freeze its weights initially and add new layers on top for classifying plant species.  \n",
        "This approach speeds up training and leverages existing visual knowledge.\n"
      ],
      "metadata": {
        "id": "1_NEiUigDY2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Build MobileNetV2 Model\n",
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=IMG_SIZE + (3,)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "C9x3MToBDZPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train the Model (Transfer Learning)\n",
        "\n",
        "We train only the top layers (with the base MobileNetV2 frozen) for several epochs.  \n",
        "This allows the new layers to learn features specific to your plant dataset.\n"
      ],
      "metadata": {
        "id": "bjm2SfLnDbTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Train Model (initial transfer learning)\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "tN8rwIwtDboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Fine-tune the Model\n",
        "\n",
        "We unfreeze the last few layers of MobileNetV2 and continue training with a lower learning rate.  \n",
        "This helps the model adapt deeper features to your specific dataset and boosts accuracy.\n"
      ],
      "metadata": {
        "id": "kEefcBKTDedA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Fine-tune (unfreeze some base layers)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "wLM6-1XGDe1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Evaluate and Visualize Results\n",
        "\n",
        "We plot training and validation accuracy, display a confusion matrix for test predictions,  \n",
        "and print a classification report to measure the modelâ€™s performance.\n"
      ],
      "metadata": {
        "id": "4ULTLG2RDh78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Evaluate and Plot Results\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "test_steps = test_generator.samples // BATCH_SIZE + 1\n",
        "predictions = model.predict(test_generator, steps=test_steps)\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, cmap='Blues', xticklabels=False, yticklabels=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "1gVWQb6cDiNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Export the Model to TensorFlow Lite (TFLite)\n",
        "\n",
        "We save the trained Keras model, then convert it to TensorFlow Lite format.  \n",
        "This `.tflite` model can be run efficiently on mobile devices (like in your Flutter app).  \n",
        "We also export a quantized (smaller/faster) version for mobile deployment.\n"
      ],
      "metadata": {
        "id": "V56M7HbPDkea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Export to TFLite (for Flutter)\n",
        "model.save('/content/plantnet_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open('/content/plantnet_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# (Optional) Quantized model:\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/content/plantnet_model_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)\n"
      ],
      "metadata": {
        "id": "7iJhzIGzDkuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Save the Class Index Mapping\n",
        "\n",
        "We save a JSON file that maps model output indices to the corresponding plant species IDs.  \n",
        "Your Flutter app will use this to translate model predictions into plant names and rich info.\n"
      ],
      "metadata": {
        "id": "5Mz2Cy2-Dmp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Save class index mapping for app\n",
        "class_indices = train_generator.class_indices  # class_name -> idx\n",
        "idx_to_species = {v: k for k, v in class_indices.items()}\n",
        "with open('/content/species_id_to_index.json', 'w') as f:\n",
        "    json.dump(idx_to_species, f)\n"
      ],
      "metadata": {
        "id": "sxYg26c5Dm9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Test Model on a Single Image\n",
        "\n",
        "We test the trained model on an individual image to verify the prediction,  \n",
        "and print the predicted species ID and name for manual inspection.\n"
      ],
      "metadata": {
        "id": "JU_X8hhCDogH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Test on Single Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img_path = '/content/drive/MyDrive/plantnet_300K/images/test/1355868/0a342112ddd74ee3ea7918c445e2133fb5b9454d.jpg'\n",
        "img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0) / 255.\n",
        "\n",
        "pred = model.predict(x)\n",
        "predicted_idx = np.argmax(pred)\n",
        "predicted_class = idx_to_species[str(predicted_idx)]\n",
        "print(\"Predicted Species ID:\", predicted_class)\n",
        "print(\"Predicted Species Name:\", species_id_to_name[predicted_class])\n"
      ],
      "metadata": {
        "id": "SrgOqByzDo0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}